{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import requests\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2014-07-01 00:00:00\n",
      "2014-08-01 00:00:00\n",
      "2014-09-01 00:00:00\n",
      "2014-10-01 00:00:00\n",
      "2014-11-01 00:00:00\n",
      "2014-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def scrape_station(station):\n",
    "    '''\n",
    "    This function scrapes the weather data web pages from wunderground.com\n",
    "    for the station you provide it.\n",
    "    You can look up your city's weather station by performing a search for\n",
    "    it on wunderground.com then clicking on the \"History\" section.\n",
    "    The 4-letter name of the station will appear on that page.\n",
    "    '''\n",
    "\n",
    "    # Scrape between July 1, 2014 and July 1, 2015\n",
    "    # You can change the dates here if you prefer to scrape a different range\n",
    "    current_date = datetime(year=2014, month=7, day=1)\n",
    "    end_date = datetime(year=2015, month=7, day=1)\n",
    "\n",
    "    # Make sure a directory exists for the station web pages\n",
    "    os.mkdir(station)\n",
    "\n",
    "    # Use .format(station, YYYY, M, D)\n",
    "    lookup_URL = 'http://www.wunderground.com/history/airport/{}/{}/{}/{}/DailyHistory.html'\n",
    "\n",
    "    while current_date != end_date:\n",
    "\n",
    "        if current_date.day == 1:\n",
    "            print(current_date)\n",
    "\n",
    "        formatted_lookup_URL = lookup_URL.format(station,\n",
    "                                                 current_date.year,\n",
    "                                                 current_date.month,\n",
    "                                                 current_date.day)\n",
    "        html = requests.get(formatted_lookup_URL).text\n",
    "\n",
    "        out_file_name = '{}/{}-{}-{}.html'.format(station, current_date.year,\n",
    "                                                  current_date.month,\n",
    "                                                  current_date.day)\n",
    "\n",
    "        with open(out_file_name, 'w') as out_file:\n",
    "            out_file.write(html)\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "\n",
    "# Scrape the stations used in this article\n",
    "for station in ['KCLT', 'KCQT', 'KHOU', 'KIND', 'KJAX',\n",
    "                'KMDW', 'KNYC', 'KPHL', 'KPHX', 'KSEA']:\n",
    "    scrape_station(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3d4fbd75444d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m for station in ['KCLT', 'KCQT', 'KHOU', 'KIND', 'KJAX',\n\u001b[1;32m    107\u001b[0m                 'KMDW', 'KNYC', 'KPHL', 'KPHX', 'KSEA']:\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mparse_station\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KCLT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-3d4fbd75444d>\u001b[0m in \u001b[0;36mparse_station\u001b[0;34m(station)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mweather_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'historyTable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wx-value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mweather_data_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'historyTable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'td'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "def parse_station(station):\n",
    "    '''\n",
    "    This function parses the web pages downloaded from wunderground.com\n",
    "    into a flat CSV file for the station you provide it.\n",
    "    Make sure to run the wunderground scraper first so you have the web\n",
    "    pages downloaded.\n",
    "    '''\n",
    "\n",
    "    # Scrape between July 1, 2014 and July 1, 2015\n",
    "    # You can change the dates here if you prefer to parse a different range\n",
    "    current_date = datetime(year=2014, month=7, day=1)\n",
    "    end_date = datetime(year=2015, month=7, day=1)\n",
    "\n",
    "    with open('{}.csv'.format(station), 'w') as out_file:\n",
    "        out_file.write('date,actual_mean_temp,actual_min_temp,actual_max_temp,'\n",
    "                       'average_min_temp,average_max_temp,'\n",
    "                       'record_min_temp,record_max_temp,'\n",
    "                       'record_min_temp_year,record_max_temp_year,'\n",
    "                       'actual_precipitation,average_precipitation,'\n",
    "                       'record_precipitation\\n')\n",
    "\n",
    "        while current_date != end_date:\n",
    "            try_again = False\n",
    "            with open('{}/{}-{}-{}.html'.format(station,\n",
    "                                                current_date.year,\n",
    "                                                current_date.month,\n",
    "                                                current_date.day)) as in_file:\n",
    "                soup = BeautifulSoup(in_file.read(), 'html.parser')\n",
    "                print(type(soup))\n",
    "                weather_data = soup.find(id='historyTable').find_all('span', class_='wx-value')\n",
    "                weather_data_units = soup.find(id='historyTable').find_all('td')\n",
    "\n",
    "                try:\n",
    "                    actual_mean_temp = weather_data[0].text\n",
    "                    actual_max_temp = weather_data[2].text\n",
    "                    average_max_temp = weather_data[3].text\n",
    "                    record_max_temp = weather_data[4].text\n",
    "                    actual_min_temp = weather_data[5].text\n",
    "                    average_min_temp = weather_data[6].text\n",
    "                    record_min_temp = weather_data[7].text\n",
    "                    record_max_temp_year = weather_data_units[\n",
    "                        9].text.split('(')[-1].strip(')')\n",
    "                    record_min_temp_year = weather_data_units[\n",
    "                        13].text.split('(')[-1].strip(')')\n",
    "\n",
    "                    actual_precipitation = weather_data[9].text\n",
    "                    if actual_precipitation == 'T':\n",
    "                        actual_precipitation = '0.0'\n",
    "                    average_precipitation = weather_data[10].text\n",
    "                    record_precipitation = weather_data[11].text\n",
    "\n",
    "                    # Verify that the parsed data is valid\n",
    "                    if (record_max_temp_year == '-1' or record_min_temp_year == '-1' or\n",
    "                            int(record_max_temp) < max(int(actual_max_temp), int(average_max_temp)) or\n",
    "                            int(record_min_temp) > min(int(actual_min_temp), int(average_min_temp)) or\n",
    "                            float(actual_precipitation) > float(record_precipitation) or\n",
    "                            float(average_precipitation) > float(record_precipitation)):\n",
    "                        raise Exception\n",
    "\n",
    "                    out_file.write('{}-{}-{},'.format(current_date.year, current_date.month, current_date.day))\n",
    "                    out_file.write(','.join([actual_mean_temp, actual_min_temp, actual_max_temp,\n",
    "                                             average_min_temp, average_max_temp,\n",
    "                                             record_min_temp, record_max_temp,\n",
    "                                             record_min_temp_year, record_max_temp_year,\n",
    "                                             actual_precipitation, average_precipitation,\n",
    "                                             record_precipitation]))\n",
    "                    out_file.write('\\n')\n",
    "                    current_date += timedelta(days=1)\n",
    "                except:\n",
    "                    # If the web page is formatted improperly, signal that the page may need\n",
    "                    # to be downloaded again.\n",
    "                    try_again = True\n",
    "\n",
    "            # If the web page needs to be downloaded again, re-download it from\n",
    "            # wunderground.com\n",
    "\n",
    "            # If the parser gets stuck on a certain date, you may need to investigate\n",
    "            # the page to find out what is going on. Sometimes data is missing, in\n",
    "            # which case the parser will get stuck. You can manually put in the data\n",
    "            # yourself in that case, or just tell the parser to skip this day.\n",
    "            if try_again:\n",
    "                print('Error with date {}'.format(current_date))\n",
    "\n",
    "                lookup_URL = 'http://www.wunderground.com/history/airport/{}/{}/{}/{}/DailyHistory.html'\n",
    "                formatted_lookup_URL = lookup_URL.format(station,\n",
    "                                                         current_date.year,\n",
    "                                                         current_date.month,\n",
    "                                                         current_date.day)\n",
    "                html = urlopen(formatted_lookup_URL).read().decode('utf-8')\n",
    "\n",
    "                out_file_name = '{}/{}-{}-{}.html'.format(station,\n",
    "                                                          current_date.year,\n",
    "                                                          current_date.month,\n",
    "                                                          current_date.day)\n",
    "\n",
    "                with open(out_file_name, 'w') as out_file:\n",
    "                    out_file.write(html)\n",
    "\n",
    "\n",
    "# Parse the stations used in this article\n",
    "for station in ['KCLT', 'KCQT', 'KHOU', 'KIND', 'KJAX',\n",
    "                'KMDW', 'KNYC', 'KPHL', 'KPHX', 'KSEA']:\n",
    "    parse_station(\"KCLT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
